{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 1 -1-Tokenizing and Padding",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHH14NnAPnry"
      },
      "source": [
        "**WORD BASED ENCODINGS:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km4YoWOcLR2H"
      },
      "source": [
        "Tensorflow and keras give us a number of ways to encode words, but the one I'm going to focus on is the tokenizer. \n",
        "This will generating the dictionary of word-encodings , creating vectors out of the sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpVgHabYOnwP",
        "outputId": "8f59ba33-c8ff-44f7-f5a6-513b585e440d"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\"I love my dog\",\n",
        "             \"I love my cat\",\n",
        "             \"You love my dog!\"]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DxwozcsDLhp"
      },
      "source": [
        "The fit on texts method of the tokenizer then takes in the data and encodes it. The tokenizer provides a word index property which returns a dictionary containing key value pairs, where the key is the word, and the value is the token for that word, which you can inspect by simply printing it out.\n",
        "\n",
        " I was capitalized, note that it's lower-cased here. That's another thing that the tokenizer does for you. It strips punctuation out. \n",
        "\n",
        " A few things to take note of; number one is that punctuation like spaces and the comma, have actually been removed. So it cleans up my text for me in that way too just to actually pull out the words. Number two, you may have noticed that I have a lowercase i here and an uppercase I here. As you can see to make a case insensitive, it's just using I and it's giving the same token for both of these.\n",
        "\n",
        "*** To note: Corpus of words***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT2pnHpjLer3"
      },
      "source": [
        "**TEXT TO SEQUENCE:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE1fgPmQPYV9"
      },
      "source": [
        "We saw how to tokenize the words and sentences, building up a dictionary of all the words to make a corpus. \n",
        "The next step will be to turn your sentences into lists of values based on these tokens. \n",
        "Once you have them, you'll likely also need to manipulate these lists, not least to make every sentence the same length, otherwise, it may be hard to train a neural network with them. \n",
        "Remember when we were doing images, we defined an input layer with the size of the image that we're feeding into the neural network. In the cases where images where differently sized, we would resize them to fit. \n",
        "Well, you're going to face the same thing with text. Fortunately, TensorFlow includes APIs to handle these issues. \n",
        "\n",
        "One really handy thing about this that you'll use later is the fact that the text to sequences called can take any set of sentences, so it can encode them based on the word set that it learned from (the one that was passed into fit on texts) and leave others. This is very significant. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDBxXyqFNncz",
        "outputId": "0288564d-f77d-4a9e-de2b-b249b641647d"
      },
      "source": [
        "#Uisng earlier defined corpus : tokenizer\n",
        "\n",
        "test_sentences = [\"I really love my dog\",\n",
        "             \"My dog loves my mantee\"]\n",
        "\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "print(test_sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 1, 2, 4], [2, 4, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1bN9gBqOG3P"
      },
      "source": [
        "Missed: really , loves , mantee : as these words were not part of word corpus\n",
        "\n",
        " instead of just ignoring unseen words, to put a special value in when an unseen word is encountered. OOV / Out of Vocabulary\n",
        "\n",
        "** Encountering Unseen Words with OOV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzsOdK29OBU9",
        "outputId": "db57fae6-24ac-4903-f536-02d09e2f162c"
      },
      "source": [
        "sentences = [\"I love my dog\",\n",
        "             \"I love my cat\",\n",
        "             \"You love my dog!\"]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(word_index)\n",
        "print(sequences)\n",
        "\n",
        "test_sentences = [\"I really love my dog\",\n",
        "             \"My dog loves my mantee\"]\n",
        "\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "print(test_sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7}\n",
            "[[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5]]\n",
            "[[4, 1, 2, 3, 5], [3, 5, 1, 3, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpQDOkKqe310"
      },
      "source": [
        "**PADDING:**\n",
        "\n",
        "As we mentioned, earlier when we were building neural networks to handle pictures. When we fed them into the network for training, we needed them to be uniform in size. Often, we use the generators to resize the image to fit for example. With texts you'll face a similar requirement before you can train with texts, we needed to have some level of uniformity of size, so padding is your friend there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5kMfx4zdtHv",
        "outputId": "38ec3bf6-9c4a-4582-b72a-77ff6a1aebdc"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog blackie who is a labraor is amazing?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "padded = pad_sequences(sequences, padding=\"post\",maxlen=7)\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "print(\"\\nSequences = \" , sequences)\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded)\n",
        "\n",
        "\n",
        "# Try with words that the tokenizer wasn't fit to\n",
        "test_data = [\n",
        "    'i really love my dog',\n",
        "    'my dog loves my place as its an open field place'\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(\"\\nTest Sequence = \", test_seq)\n",
        "\n",
        "padded = pad_sequences(test_seq)\n",
        "print(\"\\nPadded Test Sequence: \")\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'is': 7, 'cat': 8, 'do': 9, 'think': 10, 'blackie': 11, 'who': 12, 'a': 13, 'labraor': 14, 'amazing': 15}\n",
            "\n",
            "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 8], [6, 3, 2, 4], [9, 6, 10, 2, 4, 11, 12, 7, 13, 14, 7, 15]]\n",
            "\n",
            "Padded Sequences:\n",
            "[[ 5  3  2  4  0  0  0]\n",
            " [ 5  3  2  8  0  0  0]\n",
            " [ 6  3  2  4  0  0  0]\n",
            " [11 12  7 13 14  7 15]]\n",
            "\n",
            "Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1, 1, 1, 1, 1, 1, 1]]\n",
            "\n",
            "Padded Test Sequence: \n",
            "[[0 0 0 0 0 0 5 1 3 2 4]\n",
            " [2 4 1 2 1 1 1 1 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCaowhqRgiHF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}